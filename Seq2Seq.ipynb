{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Sequence to sequence model for making second part of ferdosi poems\n",
        "              Adel Parvizi"
      ],
      "metadata": {
        "id": "4WFRs6jJpfig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part 1"
      ],
      "metadata": {
        "id": "0nCIr5hTG4kQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing needed libreries"
      ],
      "metadata": {
        "id": "yKT0ZZulIQ7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch\n",
        "import time\n",
        "import torchtext\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from collections import Counter\n",
        "import os\n",
        "import os.path\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "K2yrA5XlprQ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06888b69-a4ec-4d5e-fb7c-ae741eada181"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "check avilable devices"
      ],
      "metadata": {
        "id": "iAiRO-LjIVGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tvJ_FXHZn7l",
        "outputId": "5210485c-6988-4bc4-83a9-12dc0539b4ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PyTorch Version : {}\".format(torch.__version__))\n",
        "print(\"TorchText Version : {}\".format(torchtext.__version__))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tQLeJCMUhtU",
        "outputId": "dd687f4c-0916-4b17-a6f1-3e60a08b7c19"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version : 2.0.1+cu118\n",
            "TorchText Version : 0.15.2+cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I've borrow this class from a site. Acually this class get a list of strings and give us a model that have two dictionary which associate words with indexes and vic versa.\n",
        "* If we have a vocabulary we'll use that if not we need to build ones.\n",
        "* At first we add \"sos\", \"eos\", \"unk\" to vocab. thay are nessesiry to be there. After that we tokenize sentences and add the tokens to vocab.\n",
        "* I've delet numbers and unwanted tokens.\n",
        "\n",
        "* add_captions: works on sentenses and after converting them to token, it will add them to vocab using \"add_word\"\n",
        "* add_word: adds tokens to vab.\n",
        "\n",
        "* __call__ get a list/single string and convert them/that to index"
      ],
      "metadata": {
        "id": "hkmZMgRhPGQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary(object):\n",
        "\n",
        "    def __init__(self, vocab_threshold, vocab_file='/content/drive/MyDrive/Colab_Notebooks/Seqence_to_Sequence/vocab.pkl',\n",
        "        start_word=\"<sos>\", end_word=\"<eos>\", unk_word=\"<unk>\",\n",
        "        poem_list='/content/annotations/captions_train2014.json', vocab_from_file=False):\n",
        "        self.vocab_threshold = vocab_threshold\n",
        "        self.vocab_file = vocab_file\n",
        "        self.start_word = start_word\n",
        "        self.end_word = end_word\n",
        "        self.unk_word = unk_word\n",
        "        self.poem_list = poem_list\n",
        "        self.vocab_from_file = vocab_from_file\n",
        "        self.get_vocab()\n",
        "\n",
        "    def get_vocab(self):\n",
        "        if os.path.exists(self.vocab_file) & self.vocab_from_file:\n",
        "            with open(self.vocab_file, 'rb') as f:\n",
        "                vocab = pickle.load(f)\n",
        "                self.word2idx = vocab.word2idx\n",
        "                self.idx2word = vocab.idx2word\n",
        "            print('Vocabulary successfully loaded from vocab.pkl file!')\n",
        "        else:\n",
        "            self.build_vocab()\n",
        "            with open(self.vocab_file, 'wb') as f:\n",
        "                pickle.dump(self, f)\n",
        "\n",
        "    def build_vocab(self):\n",
        "        self.init_vocab()\n",
        "        self.add_word(self.start_word)\n",
        "        self.add_word(self.end_word)\n",
        "        self.add_word(self.unk_word)\n",
        "        self.add_captions()\n",
        "\n",
        "    def init_vocab(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def add_captions(self):\n",
        "        counter = Counter()\n",
        "        for i, poem in enumerate(self.poem_list):\n",
        "            # Remove punctuation by using regex\n",
        "            rp_poem = re.sub(r'[^\\w\\s]','', poem)\n",
        "            #remove numbers\n",
        "            rnp_poem = re.sub(r'\\d+', '', rp_poem)\n",
        "            tokens = nltk.tokenize.word_tokenize(rnp_poem.lower())\n",
        "            counter.update(tokens)\n",
        "\n",
        "            if i % 20000 == 0:\n",
        "                print(\"[%d/%d] Tokenizing captions...\" % (i, len(self.poem_list)))\n",
        "\n",
        "        words = [word for word, cnt in counter.items() if cnt >= self.vocab_threshold]\n",
        "\n",
        "        for i, word in enumerate(words):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def __call__(self, word):\n",
        "      if type(word) == str:\n",
        "        if not word in self.word2idx:\n",
        "            return self.word2idx[self.unk_word]\n",
        "        return self.word2idx[word]\n",
        "      if type(word) == list:\n",
        "        a = []\n",
        "        for w in word:\n",
        "          if not w in self.word2idx : a.append(self.word2idx[self.unk_word])\n",
        "          else : a.append(self.word2idx[w])\n",
        "        return a\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)"
      ],
      "metadata": {
        "id": "-8g0Nm3yZfFt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Colab_Notebooks/Seqence_to_Sequence/ferdousi.txt') as f:\n",
        "  ferdousi_text = [line.strip() for line in f.readlines()]\n",
        "ferdousi_text = ferdousi_text[2:]\n",
        "print(f\"num of beyt\", len(ferdousi_text))\n",
        "input_poems = [ferdousi_text[i] for i in range(len(ferdousi_text)) if i % 2 == 0]\n",
        "output_poems = [ferdousi_text[i] for i in range(len(ferdousi_text)) if i % 2 == 1]\n",
        "input_poems, output_poems = input_poems[0:49608], output_poems[0:49608]\n",
        "\n",
        "vocabulary = Vocabulary(\n",
        "    vocab_threshold=0,\n",
        "    vocab_file='/content/drive/MyDrive/Colab_Notebooks/Seqence_to_Sequence/vocab.pkl',\n",
        "    poem_list = input_poems + output_poems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LdkwI1Z-2_E",
        "outputId": "ab26c90d-0429-4485-9331-cbdcbc66f271"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num of beyt 99217\n",
            "[0/99216] Tokenizing captions...\n",
            "[20000/99216] Tokenizing captions...\n",
            "[40000/99216] Tokenizing captions...\n",
            "[60000/99216] Tokenizing captions...\n",
            "[80000/99216] Tokenizing captions...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing our vocab"
      ],
      "metadata": {
        "id": "T-MERsYJRo2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = \"به نام خداوند جان و خرد کزین برتر اندیشه برنگذرد\"\n",
        "tokens = nltk.tokenize.word_tokenize(str(a).lower())\n",
        "b = [vocabulary(token) for token in tokens]\n",
        "print(a)\n",
        "print(b)\n",
        "print(\"len of vocabulary: \", len(vocabulary))\n",
        "print(vocabulary.idx2word[0])\n",
        "print(vocabulary.idx2word[1])\n",
        "print(vocabulary.idx2word[2])\n",
        "print(vocabulary.idx2word[3])\n",
        "print(vocabulary.idx2word[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxL2TvCDaYkH",
        "outputId": "4f01a3d9-57a1-4060-e8a7-6a37000de870"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "به نام خداوند جان و خرد کزین برتر اندیشه برنگذرد\n",
            "[3, 4, 5, 6, 7, 8, 394, 60, 23, 7229]\n",
            "len of vocabulary:  17778\n",
            "<sos>\n",
            "<eos>\n",
            "<unk>\n",
            "به\n",
            "نام\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary(\"برنگذرد\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bui5n1VDdi12",
        "outputId": "20671a1b-3f88-4fee-8d0c-a1d9cfe76056"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7229"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spliting data to train, test and val"
      ],
      "metadata": {
        "id": "7U5SNbUISr94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, X_test, y, y_test = train_test_split(input_poems, output_poems, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "train_data = [[X_train[i], y_train[i]] for i in range(len(X_train))]\n",
        "test_data = [[X_test[i], y_test[i]] for i in range(len(X_test))]\n",
        "val_data = [[X_val[i], y_val[i]] for i in range(len(X_val))]\n",
        "del input_poems, output_poems, X, X_test, y, y_test, X_train, X_val, y_train, y_val"
      ],
      "metadata": {
        "id": "9s2-JGQi_Mys"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train length\", len(train_data),\"\\ntest length\", len(test_data), \"\\nval length\", len(val_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "el4FVCfnCVVF",
        "outputId": "09539243-b5b1-40d9-ec00-95b7c5a0ca1b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train length 29764 \n",
            "test length 9922 \n",
            "val length 9922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probing lengt of sentence(mesra')"
      ],
      "metadata": {
        "id": "7b7lGanlTDg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = [1,2,3]\n",
        "a.reverse()\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wil52xK-c-Mx",
        "outputId": "844a6106-aace-482e-f57a-d5678386e8eb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 2, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s =[]\n",
        "for i in ferdousi_text:\n",
        "  s.append(len(nltk.tokenize.word_tokenize(str(i).lower())))\n",
        "res = np.std(s)\n",
        "mn = np.mean(s)\n",
        "print(mn, res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCuZN9ohEvMT",
        "outputId": "abd51913-6615-43ac-9fd9-7f85a3b5cd03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.730721549734421 1.0898380315180556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constructing Dataloader.\n",
        "vectorize_batch coverts sentenses to token, add start and ending token to sentenses. Also it reverses sequense of tokens. Because it make network to have a beter training process\n",
        "\n",
        "* I'm not sure that all the X and Y should be reversed or not"
      ],
      "metadata": {
        "id": "d3BmFoWET2Vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_batch(batch):\n",
        "    length = 9\n",
        "    X, Y = list(zip(*batch))\n",
        "    X = [vocabulary(nltk.tokenize.word_tokenize(sample)) for sample in X]\n",
        "    [x.reverse() for x in X]\n",
        "    X = [sample + ([2] * (length - len(sample))) if len(sample) < length else sample[:length] for sample in X]\n",
        "    Y = [vocabulary(nltk.tokenize.word_tokenize(sample)) for sample in Y]\n",
        "    [y.reverse() for y in Y]\n",
        "    Y = [sample+([2]* (length-len(sample))) if len(sample)<length else sample[:length] for sample in Y]\n",
        "    [x.append(1) for x in X]\n",
        "    [y.append(1) for y in Y]\n",
        "    [x.insert(0, 0) for x in X]\n",
        "    [y.insert(0, 0) for y in Y]\n",
        "    return torch.tensor(X, dtype=torch.int32).transpose(0, 1).to(device), torch.tensor(Y, dtype=torch.int32).transpose(0, 1).to(device)\n",
        "train_data, test_data, val_data = to_map_style_dataset(train_data), to_map_style_dataset(test_data), to_map_style_dataset(val_data)\n",
        "train_loader = DataLoader(train_data, batch_size=512, collate_fn=vectorize_batch)\n",
        "test_loader  = DataLoader(test_data, batch_size=1, collate_fn=vectorize_batch)\n",
        "val_loader  = DataLoader(val_data, batch_size=512, collate_fn=vectorize_batch)"
      ],
      "metadata": {
        "id": "GL2V45Wpeb6p"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cheking that we have done well"
      ],
      "metadata": {
        "id": "sRKSsHN3U-5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kpz8rs3SbS8r",
        "outputId": "cca37299-c4c1-40ea-afb5-7621d2151e3d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = iter(train_loader)"
      ],
      "metadata": {
        "id": "RCmyOTxUw8RI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_batch = next(data)"
      ],
      "metadata": {
        "id": "ixMjbw3acIGw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(first_batch[0])\n",
        "print(first_batch[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5uYzRaZxmbI",
        "outputId": "27bedb1c-99c2-4607-e4aa-24929c84b556"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     0,     0,  ...,     0,     0,     0],\n",
            "        [  622, 10182,   583,  ...,  2149,   411,   841],\n",
            "        [  371,   199,   188,  ...,   726,   202,  6166],\n",
            "        ...,\n",
            "        [    2,     2,     2,  ...,     2,     2,     2],\n",
            "        [    2,     2,     2,  ...,     2,     2,     2],\n",
            "        [    1,     1,     1,  ...,     1,     1,     1]], device='cuda:0',\n",
            "       dtype=torch.int32)\n",
            "torch.Size([11, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for X, Y in train_loader:\n",
        "    print(X.shape, Y.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M73L_oFH2Cqa",
        "outputId": "885f49fc-9feb-4530-ff6e-a2d89ddca700"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([11, 512]) torch.Size([11, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_batch[0][:,0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiFkNJRr2ahg",
        "outputId": "c686610c-2a1c-454f-ebd3-22a58eacfcce"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([11])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary.idx2word[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "sEGlrOuQavtk",
        "outputId": "b3dcf10a-d913-491f-dc34-f78ce1899aa3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'نام'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "wSTkDVLxbs6h"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = []\n",
        "for i in second_batch[0][:,1]:\n",
        "  print(int(i))\n",
        "  s.append(vocabulary.idx2word[int(i)])\n",
        "print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9F2BLkjk2jRe",
        "outputId": "e37ad416-4488-435d-b090-728fae627ba7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "10182\n",
            "199\n",
            "58\n",
            "509\n",
            "83\n",
            "21\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "['<sos>', 'کارمنست', 'نه', 'این', 'شاه', 'گفت', 'بدو', '<unk>', '<unk>', '<unk>', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder is used to covert our embedding data to a spaca that we dont know what is it\n",
        "\n",
        "* emb_dim: embedding dimention\n",
        "\n",
        "* hid_dim: hidden dimension\n",
        "\n",
        "* n_layers: number of layer"
      ],
      "metadata": {
        "id": "agrHitzLVjw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, src):\n",
        "\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        return hidden, cell"
      ],
      "metadata": {
        "id": "ODcx_cKE22jd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder is bulit using LSTM and a linear layer to creat our embedding data"
      ],
      "metadata": {
        "id": "eQubcvlTWu1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.probs = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        return self.probs(prediction), hidden, cell"
      ],
      "metadata": {
        "id": "OxlFJolCWVVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \"Encoder and decoder must have equal number of layers!\"\n",
        "    def init_hidden(self, trg_shape):\n",
        "\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        batch_size = trg_shape[1]\n",
        "        trg_len = trg_shape[0]\n",
        "        return torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "\n",
        "        trg_shape = trg.shape\n",
        "        trg_len = trg_shape[0]\n",
        "        outputs = self.init_hidden(trg_shape)\n",
        "        hidden, cell = self.encoder(src)\n",
        "        input = trg[0,:]\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "X2ptYMNNZaco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 71\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "input_dim = len(vocabulary)\n",
        "output_dim = len(vocabulary)\n",
        "enc_emb_dim = 256\n",
        "dec_emb_dim = 256\n",
        "hid_dim = 512\n",
        "n_layer = 2\n",
        "enc_dropout = 0.5\n",
        "dec_dropout = 0.5\n",
        "\n",
        "enc = Encoder(input_dim, enc_emb_dim, hid_dim, n_layer, enc_dropout)\n",
        "dec = Decoder(output_dim, dec_emb_dim, hid_dim, n_layer, dec_dropout)\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "id": "7LZoZO8FaB_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0881fa9-09fd-4cc8-94e9-30336f591faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(17778, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(17778, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=512, out_features=17778, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (probs): Softmax(dim=1)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgdAYkxMalu6",
        "outputId": "2aa44ba7-2417-48e5-b647-c05d6498af69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 25,578,866 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "target_padding_index = vocabulary(\"<unk>\")\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = target_padding_index).to(device)"
      ],
      "metadata": {
        "id": "xF7EZykaaxXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(first_batch[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N53u5sV4Uw6s",
        "outputId": "63a788da-82f8-4ffc-e0be-29cca1845e30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    ns = 0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src, trg = batch\n",
        "            output = model(src, trg, 0)\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:,].reshape(output.shape[0])\n",
        "            trg = trg.type(torch.LongTensor).to(device)\n",
        "            loss = criterion(output, trg)\n",
        "            correct += (output.argmax(1) == trg).type(torch.float).sum().item()\n",
        "            ns += src.shape[1]\n",
        "            epoch_loss += loss.item()\n",
        "    correct /= ns\n",
        "    return correct , epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    epoch_loss = 0\n",
        "    ns = 0\n",
        "    aize = len(iterator)\n",
        "    correct = 0\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src, trg = batch\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:,].view(-1, output_dim)\n",
        "        trg = trg[1:,].reshape(output.shape[0])\n",
        "        trg = trg.type(torch.LongTensor).to(device)\n",
        "        loss = criterion(output, trg)\n",
        "        correct += (output.argmax(1) == trg).type(torch.float).sum().item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        ns += src.shape[1]\n",
        "        epoch_loss += loss.item()\n",
        "    correct /= ns\n",
        "    return correct, epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "7DCnT1C8b708"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "qhNbBDRkb_7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "train_history = []\n",
        "val_history = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    model.train()\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_acc, train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    val_acc, valid_loss = evaluate(model, val_loader, criterion)\n",
        "    train_history += [train_loss]\n",
        "    val_history += [valid_loss]\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Train acc: {train_acc:.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f} | Val acc: {val_acc:.3f}')\n",
        "plt.plot(range(epoch+1), train_history)\n",
        "plt.plot(range(epoch+1), val_history)\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "ZJBA90zzcDx0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "outputId": "315c84c5-87c2-48e0-c6f4-209c5037ccb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 0m 29s\n",
            "\tTrain Loss: 9.636 | Train PPL: 15299.318 | Train acc: 1.128\n",
            "\t Val. Loss: 9.594 |  Val. PPL: 14675.644 | Val acc: 1.289\n",
            "Epoch: 02 | Time: 0m 29s\n",
            "\tTrain Loss: 9.595 | Train PPL: 14688.745 | Train acc: 1.284\n",
            "\t Val. Loss: 9.594 |  Val. PPL: 14674.309 | Val acc: 1.289\n",
            "Epoch: 03 | Time: 0m 29s\n",
            "\tTrain Loss: 9.595 | Train PPL: 14688.235 | Train acc: 1.284\n",
            "\t Val. Loss: 9.594 |  Val. PPL: 14674.225 | Val acc: 1.289\n",
            "Epoch: 04 | Time: 0m 29s\n",
            "\tTrain Loss: 9.595 | Train PPL: 14688.158 | Train acc: 1.284\n",
            "\t Val. Loss: 9.594 |  Val. PPL: 14674.150 | Val acc: 1.289\n",
            "Epoch: 05 | Time: 0m 29s\n",
            "\tTrain Loss: 9.595 | Train PPL: 14688.036 | Train acc: 1.284\n",
            "\t Val. Loss: 9.594 |  Val. PPL: 14674.058 | Val acc: 1.289\n",
            "Epoch: 06 | Time: 0m 29s\n",
            "\tTrain Loss: 9.595 | Train PPL: 14687.619 | Train acc: 1.284\n",
            "\t Val. Loss: 9.594 |  Val. PPL: 14673.966 | Val acc: 1.289\n",
            "Epoch: 07 | Time: 0m 29s\n",
            "\tTrain Loss: 9.595 | Train PPL: 14687.434 | Train acc: 1.284\n",
            "\t Val. Loss: 9.594 |  Val. PPL: 14673.950 | Val acc: 1.289\n",
            "Epoch: 08 | Time: 0m 29s\n",
            "\tTrain Loss: 9.595 | Train PPL: 14687.348 | Train acc: 1.284\n",
            "\t Val. Loss: 9.594 |  Val. PPL: 14674.015 | Val acc: 1.289\n",
            "Epoch: 09 | Time: 0m 29s\n",
            "\tTrain Loss: 9.595 | Train PPL: 14687.311 | Train acc: 1.284\n",
            "\t Val. Loss: 9.594 |  Val. PPL: 14674.075 | Val acc: 1.289\n",
            "Epoch: 10 | Time: 0m 29s\n",
            "\tTrain Loss: 9.595 | Train PPL: 14687.304 | Train acc: 1.284\n",
            "\t Val. Loss: 9.594 |  Val. PPL: 14674.255 | Val acc: 1.289\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 203
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "acc, test_loss = evaluate(model, test_loader, criterion)"
      ],
      "metadata": {
        "id": "88olozMHH-iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSRCdzKLKZVZ",
        "outputId": "981cd442-cd06-4ea4-bd53-473b55c6b188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Test Loss: 9.592 | Test PPL: 14654.234 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part b"
      ],
      "metadata": {
        "id": "HS9vZ0cxlDTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hid_dim * 2, hid_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, bidirectional= True)\n",
        "    def forward(self, src):\n",
        "\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "AhiELBkPm5W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
        "\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
        "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.probs = nn.Softmax(dim=1)\n",
        "    def forward(self, input, hidden, context):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # print(\"embedded shape\", embedded.shape, \"context\",  context.shape)\n",
        "        emb_con = torch.cat((embedded, context), dim = 2)\n",
        "        output, hidden = self.rnn(emb_con, hidden)\n",
        "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), dim = 1)\n",
        "        prediction = self.fc_out(output)\n",
        "        return prediction, hidden"
      ],
      "metadata": {
        "id": "A-e4hk15raPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.probs = nn.Softmax(dim=1)\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "    def init_hidden(self, trg_shape):\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        batch_size = trg_shape[1]\n",
        "        trg_len = trg_shape[0]\n",
        "        return torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_shape = trg.shape\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        outputs = self.init_hidden(trg_shape)\n",
        "        context = self.encoder(src)\n",
        "        context = context.unsqueeze(0)\n",
        "        hidden = context\n",
        "        input = trg[0,:]\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden = self.decoder(input, hidden, context)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "-SLP5K2AtJG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 68\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "input_dim = len(vocabulary)\n",
        "output_dim = len(vocabulary)\n",
        "enc_emb_dim = 256\n",
        "dec_emb_dim = 256\n",
        "hid_dim = 512\n",
        "enc_dropout = 0.5\n",
        "dec_dropout = 0.5\n",
        "\n",
        "enc = Encoder(len(vocabulary), 256, 512, 0.5)\n",
        "dec = Decoder(len(vocabulary), 256, 512, 0.5)\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apbvYnBwur-s",
        "outputId": "be76ec7c-295f-43d6-ebed-0b49d7f5b0d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(17778, 256)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (rnn): GRU(256, 512, bidirectional=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(17778, 256)\n",
              "    (rnn): GRU(768, 512)\n",
              "    (fc_out): Linear(in_features=1280, out_features=17778, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (probs): Softmax(dim=1)\n",
              "  )\n",
              "  (probs): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 265
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slZO_i7Evbpe",
        "outputId": "c86c59d3-2c79-4496-b4f7-8e22a392be8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 36,735,346 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "target_padding_index = vocabulary(\"<unk>\")\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = target_padding_index).to(device)"
      ],
      "metadata": {
        "id": "7Wcd5gYDvqYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "qfq1LBedzo22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    ns = 0\n",
        "    aize = len(iterator)\n",
        "    correct = 0\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src, trg = batch\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:,].view(-1, output_dim)\n",
        "        trg = trg[1:,].reshape(output.shape[0])\n",
        "        trg = trg.type(torch.LongTensor).to(device)\n",
        "        loss = criterion(output, trg)\n",
        "        correct += (output.argmax(1) == trg).type(torch.float).sum().item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        ns += src.shape[1]\n",
        "        epoch_loss += loss.item()\n",
        "    correct /= ns\n",
        "    return correct, epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    ns = 0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src, trg = batch\n",
        "            output = model(src, trg, 0)\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:,].reshape(output.shape[0])\n",
        "            trg = trg.type(torch.LongTensor).to(device)\n",
        "            loss = criterion(output, trg)\n",
        "            correct += (output.argmax(1) == trg).type(torch.float).sum().item()\n",
        "            ns += src.shape[1]\n",
        "            epoch_loss += loss.item()\n",
        "    correct /= ns\n",
        "    return correct , epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "H6RkmiDm8nK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_batch(batch):\n",
        "    length = 10\n",
        "    X, Y = list(zip(*batch))\n",
        "    X = [vocabulary(nltk.tokenize.word_tokenize(sample)) for sample in X]\n",
        "    X = [sample+([2]* (length-len(sample))) if len(sample) < length else sample[:length] for sample in X]\n",
        "    Y = [vocabulary(nltk.tokenize.word_tokenize(sample)) for sample in Y]\n",
        "    Y = [sample+([2]* (length-len(sample))) if len(sample)<length else sample[:length] for sample in Y]\n",
        "    [x.append(1) for x in X]\n",
        "    [y.append(1) for y in Y]\n",
        "    [x.insert(0,0) for x in X]\n",
        "    [y.insert(0,0) for y in Y]\n",
        "    return torch.tensor(X, dtype=torch.int32).transpose(0, 1).to(device), torch.tensor(Y, dtype=torch.int32).transpose(0, 1).to(device)\n",
        "train_data, test_data, val_data = to_map_style_dataset(train_data), to_map_style_dataset(test_data), to_map_style_dataset(val_data)\n",
        "train_loader = DataLoader(train_data, batch_size=512, collate_fn=vectorize_batch)\n",
        "test_loader  = DataLoader(test_data, batch_size=1, collate_fn=vectorize_batch)\n",
        "val_loader  = DataLoader(val_data, batch_size=512, collate_fn=vectorize_batch)"
      ],
      "metadata": {
        "id": "YpPwapayLaTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "train_history = []\n",
        "val_history = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_acc, train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    model.eval()\n",
        "    val_acc, valid_loss = evaluate(model, val_loader, criterion)\n",
        "    train_history += [train_loss]\n",
        "    val_history += [valid_loss]\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Train acc: {train_acc:.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f} | Val acc: {val_acc:.3f}')\n",
        "plt.plot(range(epoch+1), train_history)\n",
        "plt.plot(range(epoch+1), val_history)\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "wkbv5cBvzpDM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "outputId": "887a230e-1645-4f72-d3d5-ce7aeb596ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 0m 35s\n",
            "\tTrain Loss: 6.620 | Train PPL: 749.776 | Train acc: 1.237\n",
            "\t Val. Loss: 6.052 |  Val. PPL: 424.929 | Val acc: 1.383\n",
            "Epoch: 02 | Time: 0m 35s\n",
            "\tTrain Loss: 5.909 | Train PPL: 368.455 | Train acc: 1.385\n",
            "\t Val. Loss: 6.027 |  Val. PPL: 414.455 | Val acc: 1.359\n",
            "Epoch: 03 | Time: 0m 35s\n",
            "\tTrain Loss: 5.841 | Train PPL: 344.229 | Train acc: 1.396\n",
            "\t Val. Loss: 6.031 |  Val. PPL: 416.029 | Val acc: 1.385\n",
            "Epoch: 04 | Time: 0m 35s\n",
            "\tTrain Loss: 5.778 | Train PPL: 323.056 | Train acc: 1.423\n",
            "\t Val. Loss: 6.026 |  Val. PPL: 414.064 | Val acc: 1.384\n",
            "Epoch: 05 | Time: 0m 35s\n",
            "\tTrain Loss: 5.719 | Train PPL: 304.670 | Train acc: 1.439\n",
            "\t Val. Loss: 6.031 |  Val. PPL: 416.292 | Val acc: 1.384\n",
            "Epoch: 06 | Time: 0m 35s\n",
            "\tTrain Loss: 5.683 | Train PPL: 293.913 | Train acc: 1.448\n",
            "\t Val. Loss: 6.037 |  Val. PPL: 418.808 | Val acc: 1.385\n",
            "Epoch: 07 | Time: 0m 36s\n",
            "\tTrain Loss: 5.641 | Train PPL: 281.659 | Train acc: 1.467\n",
            "\t Val. Loss: 6.038 |  Val. PPL: 419.111 | Val acc: 1.384\n",
            "Epoch: 08 | Time: 0m 35s\n",
            "\tTrain Loss: 5.604 | Train PPL: 271.408 | Train acc: 1.487\n",
            "\t Val. Loss: 6.038 |  Val. PPL: 419.110 | Val acc: 1.384\n",
            "Epoch: 09 | Time: 0m 35s\n",
            "\tTrain Loss: 5.576 | Train PPL: 264.141 | Train acc: 1.495\n",
            "\t Val. Loss: 6.034 |  Val. PPL: 417.425 | Val acc: 1.385\n",
            "Epoch: 10 | Time: 0m 35s\n",
            "\tTrain Loss: 5.546 | Train PPL: 256.137 | Train acc: 1.504\n",
            "\t Val. Loss: 6.286 |  Val. PPL: 536.964 | Val acc: 1.385\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 271
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "acc_test, test_loss = evaluate(model, test_loader, criterion)"
      ],
      "metadata": {
        "id": "setEzNBZ0ewd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "metadata": {
        "id": "FyWD_ddf4tQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "از نتایج به دست آمده در دو قسمت بالا مشخص میشود استفاده از واحدهای جی.آر.یو علی رغم داشتن سادگی و نداشتن مشکلات از بین رفتن یا بزرگ شدن گرادیان به لاس کمتری منجر شد. در بالا از ایده ی دیگری که در لینک زیر وجود داشت نیز استافاده شد. و در واقع در این رویکرد خروجی انکدر به ورودی های واحد های جی.آر.یو و لایه ی تماما داده شد."
      ],
      "metadata": {
        "id": "VGZ71XtL0JNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/bentrevett/pytorch-seq2seq\n",
        "\n",
        "https://github.com/sauravraghuvanshi/Udacity-Computer-Vision-Nanodegree-Program/tree/master/localization_exercises"
      ],
      "metadata": {
        "id": "G-8a4Xs3JCGf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_OHKzklMwqRm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}